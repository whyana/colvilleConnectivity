---
title: "1_ColvilleDeltaConnectivityAnalysis_classGeneration"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Libraries
```{r}
library(tidyverse)
library(sf)
library(lubridate)
library(grDevices)
library(mapview)
library(extrafont)
library(ggpubr)
library(ggmap)
library(RgoogleMaps)
library(broom)
library(HistDAWass)
library(dataRetrieval) # only used for discharge section
library(RgoogleMaps)
library(sp)
```

# Functions
```{r}
#takes RGB to calculate dominant wavelength
chroma <- function(R, G, B) {  
  require(colorscience)

# Convert R,G, and B spectral reflectance to dominant wavelength #based
# on CIE chromaticity color space

# see Wang et al 2015. MODIS-Based Radiometric Color Extraction and
# Classification of Inland Water With the Forel-Ule
# Scale: A Case Study of Lake Taihu

# chromaticity.diagram.color.fill()
Xi <- 2.7689*R + 1.7517*G + 1.1302*B
Yi <- 1.0000*R + 4.5907*G + 0.0601*B
Zi <- 0.0565*G + 5.5943*B

# calculate coordinates on chromaticity diagram
x <-  Xi / (Xi + Yi +  Zi)
y <-  Yi / (Xi + Yi +  Zi)
z <-  Zi / (Xi + Yi +  Zi)

# calculate hue angle
alpha <- atan2( (x - (1/3)), (y - (1/3))) * 180/pi

# make look up table for hue angle to wavelength conversion
cie <- cccie31 %>%
  dplyr::mutate(a = atan2( (x - (1/3)), (y - (1/3))) * 180/pi) %>%
  dplyr::filter(wlnm <= 700) %>%
  dplyr::filter(wlnm >=380) 

# find nearest dominant wavelength to hue angle
wl <- cie[as.vector(sapply(alpha,function(x) which.min(abs(x - cie$a)))), 'wlnm']

return(wl)
}
```

# Imports and file paths
```{r}
# Names of files and folders for reflectance data
import.filePath = "E:/Research/DeltaicConnectivity/ColvilleDelta/Data/DataDownloads"
import.Lakes = "ColvilleDataExport_20210210_75pctCloud.csv"
import.Channel = "ColvilleApexDataExport_20210210_75pctCloud.csv"

#Name of file and folder for classification export
export.filePath = "E:/Research/DeltaicConnectivity/ColvilleDelta/Data/Outputs"
export.fileName = "Colville_Classified_data_20210303"

#Name of file and folder for lake shapefiles
shapeFiles.filePath = "E:/Research/DeltaicConnectivity/ColvilleDelta/Data/Shapefiles"
lakes.shapeFile = "ColvilleShapefilesEdited.shp"
setwd(shapeFiles.filePath)
lakes.sf = st_read(lakes.shapeFile)

# Name of file and folder for GECI validation data
valFile.path="E:/Research/DeltaicConnectivity/ColvilleDelta/Data/DataDownloads"
valFileName = "colvilleValidation20200508.csv"

# Name of file and folder for figures
figures.filePath = "C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Papers/ColvilleDeltaConnectivity/Figures/Figures_raw"
dens.figure.name = "densityPlotExample.pdf"
clust.figure.name = "clusteringExample.pdf"

```

# Calculate top and bottom discharge years during 2003-2019 
```{r}
siteNumber="15875000" #ID number for the USGS gage at Umiat, AK
ColvilleInfo <- readNWISsite(siteNumber)
parameterCd <- "00060"
yearlySummerSummary <- readNWISdv(siteNumber,parameterCd,
                      "2000-01-01","2019-12-31") %>% 
  mutate(date = as_date(Date),
         month = month(date),
         doy = yday(date),
         year=year(date)) %>% as_tibble() %>% rename(discharge = X_00060_00003) %>% 
  mutate(time_period = case_when(
      year<2005 ~ "2000-2004",
      year>=2005 & year < 2010 ~ "2005-2009",
      year >= 2010 & year <2015 ~ "2010-2014",
      year >=2015 ~ "2015-2019"
    )) %>%   filter(month>=6 & month <=9) %>% group_by(year) %>% 
  summarise(mean_discharge=mean(discharge),
            total_discharge=sum(discharge), 
            max_discharge = max(discharge)) %>% ungroup()%>%  filter(year!=2002) # remove 2002 because we don't have data from that year in June

maxSummer = yearlySummerSummary %>% arrange(desc(max_discharge))
high_maxSummer = maxSummer[1:5,]$year
low_maxSummer = maxSummer[(length(maxSummer$year)-4):length(maxSummer$year), ]$year
```

# Import and filter the lake and channel data  - calculates both for mean, tenth percentile and 90% dom wv values.
```{r}
# Import lake and channel data
setwd(import.filePath)
all.lakes = read.csv(import.Lakes) %>% dplyr::as_tibble()
all.channels = read.csv(import.Channel) %>% dplyr::as_tibble()

# Filter lake and channel data
lakes.filter = all.lakes %>% 
  mutate(dateTime = as_datetime(system.time_start_mean*0.001),
         date = as_date(dateTime)) %>% 
  select(-system.time_start_mean, -count) %>% 
  filter(Red_count >= 100)  # each observation must be made up of at least 100 good cloud-free pixels
channels.filter = all.channels %>% 
  filter(Red_count>=100) %>% # each observation must be made up of at least 100 good cloud-free pixels
  mutate(dateTime = as_datetime(system.time_start_mean*0.001),
         date=as_date(dateTime)) %>% 
  select(-system.time_start_mean)

# Calculate dominant wavelength for lake and channel observations
## Lakes
# mean
m_lake_r = lakes.filter$Red_mean/1000
m_lake_g = lakes.filter$Green_mean/1000
m_lake_b = lakes.filter$Blue_mean/1000
dom_wv_lake_m = chroma(m_lake_r,m_lake_g,m_lake_b)
#p 10
p10_lake_r = lakes.filter$Red_p10/1000
p10_lake_g = lakes.filter$Green_p10/1000
p10_lake_b = lakes.filter$Blue_p10/1000
dom_wv_lake_p10 = chroma(p10_lake_r,p10_lake_g,p10_lake_b)
#p90
p90_lake_r = lakes.filter$Red_p90/1000
p90_lake_g = lakes.filter$Green_p90/1000
p90_lake_b = lakes.filter$Blue_p90/1000
dom_wv_lake_p90 = chroma(p90_lake_r,p90_lake_g,p90_lake_b)

lakes.combo = cbind.data.frame(lakes.filter, dom_wv_lake_m, dom_wv_lake_p10, dom_wv_lake_p90) %>% as_tibble() 
## Channel
#mean
m_channel_r = channels.filter$Red_mean/1000
m_channel_g = channels.filter$Green_mean/1000
m_channel_b = channels.filter$Blue_mean/1000
dom_wv_chan_m = chroma(m_channel_r,m_channel_g,m_channel_b)
# p10
p10_channel_r = channels.filter$Red_p10/1000
p10_channel_g = channels.filter$Green_p10/1000
p10_channel_b = channels.filter$Blue_p10/1000
dom_wv_chan_p10 = chroma(p10_channel_r,p10_channel_g,p10_channel_b)
#p90
p90_channel_r = channels.filter$Red_p90/1000
p90_channel_g = channels.filter$Green_p90/1000
p90_channel_b = channels.filter$Blue_p90/1000
dom_wv_chan_p90 = chroma(p90_channel_r,p90_channel_g,p90_channel_b)

channels.combo = cbind.data.frame(channels.filter, dom_wv_chan_m, dom_wv_chan_p10, dom_wv_chan_p90) %>% as_tibble() %>% 
  rename(delta=name, 
                 Blue_chan_m = Blue_mean,Blue_chan_p10 = Blue_p10,Blue_chan_p90 = Blue_p90 ,
                 Gb_chan_m = Gb_ratio_mean, Gb_chan_p10 = Gb_ratio_p10,Gb_chan_p90 = Gb_ratio_p90,
                 Green_chan_m=Green_mean,Green_chan_p10=Green_p10,Green_chan_p90=Green_p90, 
                 Ndssi_chan_m=Ndssi_mean,Ndssi_chan_p10=Ndssi_p10, Ndssi_chan_p90=Ndssi_p90, 
                 Nsmi_chan_m=Nsmi_mean,Nsmi_chan_p10=Nsmi_p10, Nsmi_chan_p90=Nsmi_p90,
                 Red_chan_m=Red_mean, Red_chan_p10=Red_p10, Red_chan_p90=Red_p90, count_chan= Red_count) 

# Join lake and channel data from the same date and calculate the dominant wavelength ratio
len_uniqueLake = function(df){
  no_unique = length(unique(df$dateTimeLake))
}
len = function(df){
  no = length(df$dateTimeLake)
}
channels.lakes.all = lakes.combo %>% 
  full_join(channels.combo, by=c("date", "delta")) %>%
  mutate(dom_wv_ratio_m = dom_wv_lake_m/dom_wv_chan_m,
         dom_wv_ratio_p10 =dom_wv_lake_p10/dom_wv_chan_p10,
         dom_wv_ratio_p90 =dom_wv_lake_p90/dom_wv_chan_p90,
         G_ratio_m = Green_mean/Green_chan_m, 
         B_ratio_m = Blue_mean/Blue_chan_m,
         R_ratio_m = Red_mean/Red_chan_m,
         Gb_ratio_m = Gb_ratio_mean/Gb_chan_m,
         Ndssi_ratio_m = Ndssi_mean/Ndssi_chan_m,
         Nsmi_ratio_m =Nsmi_mean/Nsmi_chan_m,
         lci_m = (dom_wv_chan_m-dom_wv_lake_m)/(dom_wv_chan_m+dom_wv_lake_m)) %>% 
  mutate(dateTimeLake=dateTime.x, dateTimeChan=dateTime.y) %>% select(-dateTime.x, -dateTime.y) %>% 
  filter(!is.na(dom_wv_ratio_m)) %>%  #Doesn't matter which joined variable you use here--just filtering those that don't have a match
  mutate(diff = abs(dateTimeLake-dateTimeChan)) %>% 
  arrange(ID, date, diff) %>% 
  group_by(ID, date) %>% nest() %>% 
  mutate(length_all = map_dbl(data, len), #Gives you the number of total matchup observations of each lake on each day (might be one lake to two channel or visa versa etc.)
         length_uniqueLake = map_dbl(data, len_uniqueLake)) %>% ungroup() # Gives you the number of unique lake observations each day

# Only keep closest matching lake and channel observation for each date

## Only one channel and one lake observation per day
best = channels.lakes.all %>% 
  filter(length_all == 1 & length_uniqueLake==1) %>% unnest(cols=data)

## More than one channel obs, but only one lake observation --> Pick the matchup with the closest overpass time
nextBest = channels.lakes.all %>% 
  filter(length_all>1 & length_uniqueLake==1) %>% 
  unnest(cols=data) %>% group_by(ID, date) %>% arrange(diff) %>% 
  mutate(id=row_number()) %>% filter(id==1) %>% select(-id)

## More than one channel obs and more than one lake obs --> pick the closest match
lessGood = channels.lakes.all %>% 
  filter(length_all>1 & length_uniqueLake>1) %>% unnest(cols=data) %>% 
  group_by(ID, dateTimeLake) %>% 
  arrange(diff) %>% mutate(id=row_number()) %>% 
  filter(id==1) %>% select(-id)

#Final dataset
channels.lakes = rbind.data.frame(best, nextBest, lessGood) %>% 
  select(-length_all, -length_uniqueLake) %>% ungroup() %>% 
  mutate(month = month(dateTimeLake),
         year=year(dateTimeLake))


```

# Enact classifications and plot Figure 3 in the paper
```{r}
# Add columns for each time period 
channels.lakes.AddCategories = channels.lakes %>% mutate(
  time_period=  case_when(
    year<2005 ~ "2000-2004",
    year>=2005 & year < 2010 ~ "2005-2009",
    year >= 2010 & year <2015 ~ "2010-2014",
    year >=2015 ~ "2015-2019"
  ),
  valYear = ifelse((year>=2013 & year<=2016), "yes", "no"),
  dischargeGroup = case_when(
    year %in% high_maxSummer ~ "high discharge",
    year %in% low_maxSummer ~"low discharge")
)

# Split into time period, validation, and time period groups
goodLakes.time = channels.lakes.AddCategories %>% 
  group_by(delta, ID, time_period) %>% summarise(no_obs = n()) %>% 
  spread(time_period, no_obs) %>%
  filter(`2000-2004`>10 & `2005-2009`>10 & `2010-2014`>10 & `2015-2019`>10) 
goodLakes.time.ids = goodLakes.time$ID
  
goodLakes.val = channels.lakes.AddCategories %>% filter(valYear=="yes") %>% 
  group_by(delta, ID) %>% summarise(no_obs = n()) %>% filter(no_obs>10)
goodLakes.val.ids = goodLakes.val$ID

goodLakes.discharge = channels.lakes.AddCategories %>% filter(!is.na(dischargeGroup)) %>% 
  group_by(delta, ID, dischargeGroup) %>% summarise(no_obs = n()) %>% 
  spread(dischargeGroup, no_obs) %>%
  filter(`high discharge`>10 & `low discharge`>10) 
goodLakes.discharge.ids = goodLakes.discharge$ID

# Make a dataframe of all of the data - grouped in how it will be classified using kmeans
time.nest = channels.lakes.AddCategories %>% filter(ID %in% goodLakes.time.ids) %>% 
  group_by(ID, time_period) %>% nest()
val.nest = channels.lakes.AddCategories %>% filter(valYear=="yes" & ID %in% goodLakes.val.ids) %>%select(-time_period) %>% 
  group_by(ID) %>% nest() %>% mutate(time_period="validation")
dis.nest = channels.lakes.AddCategories %>% filter(!is.na(dischargeGroup) & ID %in% goodLakes.discharge.ids)%>% select(-time_period) %>% 
  group_by(ID, dischargeGroup) %>% nest() %>%  rename(time_period = dischargeGroup)

all.nest = rbind.data.frame(time.nest, val.nest, dis.nest)


# Apply the clustering algorithm for all.
nest.len = length(all.nest$ID)
lod = vector("list",nest.len)
for (z in 1:nest.len){
  dat = all.nest$data[[z]]
  v1 = dat$dom_wv_ratio_m
  lod[[z]] = data2hist(v1, type="regular")
  }
myMat = new("MatH", nrows=nest.len, ncols=1, ListOfDist=lod, names.rows=c(1:nest.len), names.cols="density") # Gets mean and standard dev of each density plot

clustered.data = WH_kmeans(myMat, k=3, verbose=F, rep=100)
class = clustered.data$solution$IDX
centers = clustered.data$solution$centers
print(centers) # because cluster (may change each run, so need to manually check) has the highest mean and smallest standard dev, it is the high connectivity group, the other two are low connectivity
cent1 = centers@M[[1]]@m
cent2 = centers@M[[2]]@m
cent3 = centers@M[[3]]@m
centerMeans = c(cent1, cent2, cent3)
highestMean.index = which.max(centerMeans)
print(clustered.data$quality)
grouped.data.m = cbind(all.nest,class )%>% as_tibble() %>% rename(class=`...4`) %>% 
  mutate(connectivity = ifelse(class==highestMean.index, "high functional connectivity", "low functional connectivity")) %>% 
  mutate(type="m")


# Get the classification file ready for export
setwd(export.filePath)
write_rds(x=grouped.data.m, path=export.fileName)

#Makes the clustering figure for the paper (Figure 3)

class1= cbind(centers@M[[1]]@x,centers@M[[1]]@p ) %>% as_tibble() %>% mutate(class="1")
class2= cbind(centers@M[[2]]@x,centers@M[[2]]@p ) %>% as_tibble()%>% mutate(class="2")
class3= cbind(centers@M[[3]]@x,centers@M[[3]]@p ) %>%as_tibble() %>% mutate(class="3")

colo = c("3"="#A6E498", "1"="#3C7A2E", "2"="#619CFF")

clust.plot=ggplot()+
  geom_line(data=class1 %>% as_tibble(), aes(x=V1, y=V2, color = class), size=2)+
  geom_line(data=class2 %>% as_tibble(), aes(x=V1, y=V2, color = class),size=2)+
  geom_line(data=class3 %>% as_tibble(), aes(x=V1, y=V2, color = class), size=2)+
  theme_bw()+
  stat_ecdf(data=grouped.data.m %>% filter(time_period=="validation") %>% unnest() %>% mutate(class=as.character(class)), aes(x=dom_wv_ratio_m, group=ID, color=class), geom="smooth", size=0.07, pad=FALSE)+
 scale_colour_manual(values=colo)+   
  theme(axis.title=element_text(family="Calibri", face="bold", size = (12)),
          axis.text =element_text(family="Calibri",size = (12)),
          strip.text = element_text(family="Calibri", size=(12), face="bold"),
           legend.text = element_text(family="Calibri", size=(12)),
           legend.title=element_blank(),
           legend.position="bottom",
           title = element_text(family="Calibri", face="bold", size=12))+xlab("dominant wavelength ratio")+ylab("cumulative probability")
#Export the plot
setwd(figures.filePath)
ggsave(filename=clust.figure.name, plot=clust.plot, width=4, height=5,  device=cairo_pdf)





```


# Example figure of density plots for each lake type (plot part of Figure 2)
```{r}
# Import the classification results for the validation time period
# GE validation
valFile.path="E:/Research/DeltaicConnectivity/ColvilleDelta/Data/DataDownloads"
valFileName = "colvilleValidation20200508.csv"
setwd(export.filePath)
classifications = read_rds(export.fileName) %>% filter(time_period=="validation")

# Import the GECI validation classification dataset
setwd(valFile.path)
lake.class = read.csv(valFileName , stringsAsFactors = F) %>% as_tibble() %>% rename(validation=Connected) %>% select(-Row.Number, -note)

# Import lake shapefiles
setwd(shapeFiles.filePath)
lakes.sf = st_read(lakes.shapeFile)

# Combine dataframes and plot example density plots for a lake with a channel vs one without during the google earth validation period (2013-2016)
exampleIDS = c("2989039_3","2926574")
examplePlot=classifications %>% left_join(lake.class, by="ID") %>% filter(ID %in% exampleIDS) %>% 
  mutate(facet_name = ifelse(ID=="2989039_3", "channel present (lake ID: 2989039_3)", "no channel present (lake ID: 2926574)")) %>% 
  unnest(data) %>% 
  ggplot()+geom_density(aes(x=dom_wv_ratio_m))+
  facet_wrap(~facet_name, nrow=2)+theme_bw()+
    theme(axis.title=element_text(family="Calibri", face="bold", size = (16)),
        axis.text =element_text(family="Calibri",size = (12)),
        strip.text = element_text(family="Calibri", size=(12), face="bold"),
        legend.text = element_text(family="Calibri", size=(12)),
        legend.title=element_blank(),
        legend.position="empty",
        title = element_text(family="Times New Roman", face="bold", size=16))+xlab("dominant wavelength ratio")+ylab("density")
# export plot
setwd(figures.filePath)
ggsave(filename=dens.figure.name, plot=examplePlot, width=4, height=5,  device=cairo_pdf)

```

